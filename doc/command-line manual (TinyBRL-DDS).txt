
                              Command-line manual

NAME
     TinyBRL-DDS -- A bayesian reinforcement learning library (by Castronovo Michael)



USAGE OVERVIEW               
     --offline_learning                                        
          --agent <string>                                     
               "agent class name"
                    --agent_file <string>                      
               RandomAgent
               OptimalAgent
               EGreedyAgent
                    --epsilon <double>                         
               SoftMaxAgent
                    --tau <double>
               VDBEEGreedyAgent
                    --sigma <double>
                    --delta <double>
                    --ini_epsilon <double>
               FormulaAgent
                    --formula <string>
                    --variables <integer> <string> ... <string>
               BAMCPAgent
                    --K <integer>
                    [--D <integer>]
               BFS3Agent
                    --K <integer>
                    --C <integer>
                    [--D <integer>]
               SBOSSAgent
                    --K <integer>
                    --delta <double>
               BEBAgent
                    --beta <double>
               OPPSDSAgent
                    --n_draws <integer>                        
                    --c <double>
                    --formula_set
                         --formula_set_file <string>
                    --variables <integer> <string> ... <string>
                    --discount_factor <double>                 
                    --horizon_limit <integer>
               OPPSCSAgent
                    --n_draws <integer>
                    --c <double>
                    --agent_factory <string>                   
                         EGreedyAgentFactory
                              --epsilon <double> <double>        
                         SoftMaxAgentFactory
                              --tau <double> <double>
                         VDBEEGreedyAgentFactory
                              --sigma <double> <double>
                              --delta <double> <double>
                              --ini_epsilon <double> <double>
                         FormulaAgentFactory
                              --weights <double> <double>
                              --variables <integer> <string> ... <string> 
                    --discount_factor <double>                 
                    --horizon_limit <integer>               
          --mdp_distribution <string>                          
               "MDP distribution class name"
                    --mdp_distribution_file <string>           
          [--compress_output]
          --output <string>                                    
     
     --new_experiment                                          
          --name <string>
          --mdp_distribution <string>                          
               "MDP distribution class name"
                    --mdp_distribution_file <string>           
          --n_mdps <integer>                                   
          --n_simulations_per_mdp <integer>                    
          --discount_factor <double>                           
          --horizon_limit <integer>                            
          [--safe_simulations]
          [--save_trajectories]
          [--compress_output]
          --output <string>                                    
     
     --run_experiment                                          
          --experiment                                         
               --experiment_file <string>                      
          --agent <string>                                     
               "agent class name"
                    --agent_file <string>                     
               RandomAgent
               OptimalAgent
          --n_threads <integer>                                   
          [--compress_output]
          --output <string> 
          --refresh_frequency <integer>                          
          --backup_frequency <integer>                           

     --formula_set_generation
          --n_variables <integer>
          --tokens <integer> <string> ... <string>
          --max_size <integer>
          [--reduce
               --n_points <integer>
               --points_range <double> <double>]
          [--compress_output]
          --output <string>

     --help


OPTIONS DESCRIPTION
     --agent <string>
          The type of Agent to load among:
               - RandomAgent
               - OptimalAgent
               - EGreedyAgent (parameters: --epsilon)
               - SoftMaxAgent (parameters: --tau)
               - VDBEEGreedyAgent (parameters: --sigma, --delta, --ini_epsilon)
               - FormulaAgent (parameters: --formula, --variables)
               - BAMCPAgent (parameters: --K [--D])
               - BFS3Agent (parameters: --K --C [--D])
               - SBOSSAgent (parameters: --K --delta)
               - BEBAgent (parameters: --beta)
               - OPPSDSAgent (parameters: --n_draws, --c, --formula_set, --variables, --discount_factor, --horizon_limit)
               - OPPSCSAgent (parameters: --n_draws, --c, --agent_factory, --discount_factor, --horizon_limit)

          Can either be followed by agent's parameters (if any) or by a file
          (--agent_file)

     --agent_factory <string>
          The type of AgentFactory to load among:
               - EGreedyAgentFactory (parameters: --epsilon, --counters)
               - SoftMaxAgentFactory (parameters: --tau, --counters)
               - VDBEEGreedyAgentFactory
                 (parameters: --sigma, --delta, --ini_epsilon, --counters)

     --agent_file <string>
          The file containing the data of the Agent to load.

     --backup_frequency <double>
          The frequency of backup creation (in s).

     --beta
          (BEBAgent parameter)
          The bonus parameter.

     --c <double>
          (OPPSDSAgent & OPPSCSAgent parameter)
          The constant to use in the UCB1 formula:
               I_t(i) = mu_i + c * sqrt(ln(n_t) / n_i)

     --C <integer>
          (BFS3Agent parameter)
          The branching factor.

     --compress_output
          If specified, the output/backup files are compressed.

     --D <integer>
          (BAMCPAgent & BFS3Agent parameter)
          The maximal depth.

     --delta <double>
          (VDBEEGreedyAgent parameter)
          Determine the influence of the selected action on the exploration
          rate.

     --delta <double> <double>
          (VDBEEGreedyAgentFactory parameter)
          The minimal and maximal value of parameter 'delta' of the
          VDBEEGreedyAgents defined by this AgentFactory.

     --delta <double>
          (SBOSSAgent parameter)
          The maximal threshold for resampling.

     --discount_factor <double>
          The discount factor.

     --epsilon <double>
          (EGreedyAgent parameter)
          The probability for the Agent to choose the next action to perform
          randomly.
     
     --epsilon <double> <double>
          (EGreedyAgentFactory parameter)
          The minimal and maximal value of parameter 'epsilon' of the
          EGreedyAgents defined by this AgentFactory.

     --experiment
          Followed by the parameters of the Experiment to load.

     --experiment_file <string>
          The file containing the data of the Experiment to load.

     --formula <string>
          A formula in RPN notation (between quotes to be parsed correctly).

     --formula_set
          Followed by the parameters of the formula set to load.

     --formula_set_file <string>
          The file containing the data of the formula set to load.

     --help
          Display this help.

     --horizon_limit <integer>
          The horizon limit.

     --ini_epsilon <double>
          (VDBEEGreedyAgent parameter)
          The initial value of epsilon for each state-action pair.

     --K <integer>
          (BAMCPAgent & BFS3Agent parameter)
          The number of nodes to expand at each time-step.

     --K <integer>
          (SBOSSAgent parameter)
          The number of model samples to take for each state-action pair.

     --max_size <integer>
          The maximal size of a formula (= the maximal number of tokens to
          combine).
     
     --mdp_distribution <string>
          The type of MDP distribution to load among:
               - DirMultiDistribution

          Can either be followed by mdp distribution's parameters (if any) or
          by a file (--mdp_distribution_file)
     
     --mdp_distribution_file <string>
          The file containing the data of the MDP distribution to load.
     
     --n_draws <integer>
          (OPPSDSAgent & OPPSCSAgent parameter)
          The number of draws of the internal UCB1/UCT.
          
     --n_mdps <integer>
          The number of MDPs to consider.
     
     --n_points <integer>
          The number of points to draw to discriminate the formulas.
     
     --n_simulations_per_mdp <integer>
          The number of trajectories per MDP to consider.

     --n_threads <integer>
          The number of threads to use.

     --n_variables <integer>
          The number of variables part of the set of tokens.

     --name <string>
          The name of the Experiment to create.

     --new_experiment
          New Experiment mode, where an Experiment is created.

     --offline_learning
          Offline Learning mode, where an Agent learns from a prior MDP
          distribution.
     
     --output <string>
          The output file.
     
     --points_range <double> <double>
          The range of value in which to draw each component of the points.
     
     --reduce
          If specified, reduce the set of formulas to generate by discarding
          the formulas which are equivalent.
          A formula F1 is equivalent to F2 iff any couple of points are sorted
          in the same way by both formula:
               F1(x1) < F1(x2) <--> F2(x1) < F2(x2)
                                or
               F1(x1) > F1(x2) <--> F2(x1) > F2(x2)
     
     --refresh_frequency <integer>
          The frequency of screen output refreshing (in s).

     --run_experiment
          Run Experiment mode, where an Agent is tested on a set of MDPs
          defined by an Experiment created previously.
     
     --safe_simulations
          If set, the MDP is 'unknown', preventing the agent to access MDP data
          (e.g.: the transition matrix).

     --save_trajectories
          If set, the created experiment will not save the complete trajectories
          of the agents tested on it (only the rewards will remain).

     --sigma <double>
          (VDBEEGreedyAgent parameter)
          The inverse sensitivity.
     
     --sigma <double> <double>
          (VDBEAgentFactory parameter)
          The minimal and maximal value of parameter 'sigma' of the
          VDBEEGreedyAgents defined by this AgentFactory.
          
     --tau
          (SoftMaxAgent parameter)
          The temperature, controlling smoothly the transition from a greedy
          behaviour (tau -> 0) to a random behaviour (tau -> inf), where the
          action which is the best according to the Agent is more likely to be
          drawn.

     --tau <double> <double>
          (SoftMaxAgentFactory parameter)
          The minimal and maximal value of parameter 'tau' of the
          SoftMaxAgents defined by this AgentFactory.

     --tokens <integer> <string> ... <string>
          Defines the set of tokens (except variables) to be used in the
          formula. The first integer is the number of tokens entered. The
          following strings are the list of symbols representing the tokens:
               <integer>    -->  A constant              (0 operand)   
               ABS          -->  The absolute value      (1 operand)
               LN           -->  The napierian logarithm (1 operand)
               SQRT         -->  The square root         (1 operand)
               INV          -->  The inverse             (1 operand)
               OPP          -->  The opposite            (1 operand)
               SUB          -->  The subtraction         (2 operands)
               DIV          -->  The division            (2 operands)
               ADD<integer> -->  The addition            (<integer> operand(s))
               MUL<integer> -->  The multiplication      (<integer> operand(s))
               MIN<integer> -->  The minimum             (<integer> operand(s))
               MAX<integer> -->  The maximum             (<integer> operand(s))
               AVG<integer> -->  The average             (<integer> operand(s))

     --variables <integer> <string> ... <string>
          (FormulaAgent, OPPSDSAgent and FormulaAgentFactory parameter)
          Defines the set of variables to be used in the formula(s). The first
          integer is the number of variables entered. The following variables
          are supported:
               QMean        --> Represents the Q-function based on the mean
                                model of a prior distribution.
               QSelf        --> Represents the Q-function based on the 'self'
                                model of a prior distribution (each state is
                                only reachable from itself).
               QUniform     --> Represents the Q-function based on the 'uniform'
                                model of a prior distribution (each state is
                                reachable from the other states).

          e.g.: --variables 2 "QUniform" "QMean"
                         ==>  X0 represents "QUniform"; X1 represents "QMean".

     --weights <double> <double>
          The minimal and maximal value of the weights of the FormulaAgents
          defined by this AgentFactory.
          
     

USAGE EXAMPLES

     --- Offline Learning mode ---
     
          ./TinyBRL-DDS --offline_learning \
               --agent EGreedyAgent --epsilon 0.25 \
               --mdp_distribution "DirMultiDistribution" \
                    --mdp_distribution_file "data/distributions/UGC-distrib.dat" \
               --output "data/agents/egreedy_agent(0.25).dat"
     
     
     --- New Experiment mode ---
     
          ./TinyBRL-DDS --new_experiment \
               --name "GC Experiment" \
               --mdp_distribution "DirMultiDistribution" \
                    --mdp_distribution_file "data/distributions/GC-distrib.dat" \
               --n_mdps 1000 --n_simulations_per_mdp 1 \
               --discount_factor 0.95 --horizon_limit 250 \
               --compress_output \
               --output "data/experiments/GC-1000-exp.dat"
     
     
     --- Run Experiment mode ---

          ./TinyBRL-DDS --run_experiment \
               --experiment \
                    --experiment_file "data/experiments/GC-1000-exp.dat.zz" \
               --agent EGreedyAgent \
                    --agent_file "data/agents/egreedy_agent(0.25).dat" \
               --n_threads 1 --compress_output \
               --output "data/results/GC-1000-egreedy_agent(0.25).dat" \
               --refresh_frequency 1 --backup_frequency 15


     --- Formula set generation mode ---

          ./TinyBRL-DDS --formula_set_generation \
               --n_variables 3 \
               --tokens 4 ADD2 42 AVG3 MUL2 \
               --max_size 5 \
               --reduce --n_points 1000 --points_range -100.0 100.0 \
               --output "data/formula_sets/fset(3,4,5).dat"

