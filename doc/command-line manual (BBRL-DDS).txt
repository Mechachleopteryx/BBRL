
                              Command-line manual

NAME
     BBRL-DDS -- A bayesian reinforcement learning library (by Castronovo Michael)



USAGE OVERVIEW               
     --offline_learning
          [--seed <integer>]                                    
          --agent <string>                                     
               "agent class name"
                    --agent_file <string>                      
               RandomAgent
               OptimalAgent
               EGreedyAgent
                    --epsilon <double>                         
               SoftMaxAgent
                    --tau <double>
               VDBEEGreedyAgent
                    --sigma <double>
                    --delta <double>
                    --ini_epsilon <double>
               FormulaAgent
                    --formula <string>
                    --variables <integer> <string> ... <string>
               BAMCPAgent
                    --K <integer>
                    [--D <integer>]
               BFS3Agent
                    --K <integer>
                    --C <integer>
                    [--D <integer>]
               SBOSSAgent
                    --epsilon <double>
                    --delta <double>
               BEBAgent
                    --beta <double>
               OPPSDSAgent
                    --n_draws <integer>                        
                    --c <double>
                    --formula_set
                         --formula_set_file <string>
                    --variables <integer> <string> ... <string>
                    --discount_factor <double>                 
                    --horizon_limit <integer>
               OPPSCSAgent
                    --n_eval <integer>
                    --K <integer>
                    [--k <integer> --h_max <integer> --delta <double>]
                    --agent_factory <string>                   
                         EGreedyAgentFactory
                              --epsilon <double> <double>        
                         SoftMaxAgentFactory
                              --tau <double> <double>
                         VDBEEGreedyAgentFactory
                              --sigma <double> <double>
                              --delta <double> <double>
                              --ini_epsilon <double> <double>
                         FormulaAgentFactory
                              --weights <double> <double>
                              --variables <integer> <string> ... <string> 
                    --discount_factor <double>                 
                    --horizon_limit <integer>               
          --mdp_distribution <string>                          
               "MDP distribution class name"
                    --mdp_distribution_file <string>           
          [--compress_output]
          --output <string>                                    
     
     --new_experiment
          [--seed <integer>]                                          
          --name <string>
          --mdp_distribution <string>                          
               "MDP distribution class name"
                    --mdp_distribution_file <string>           
          --n_mdps <integer>                                   
          --n_simulations_per_mdp <integer>                    
          --discount_factor <double>                           
          --horizon_limit <integer>                            
          [--safe_simulations]
          [--save_trajectories]
          [--compress_output]
          --output <string>                                    
     
     --run_experiment
          [--seed <integer>]                                          
          --experiment                                         
               --experiment_file <string>                      
          --agent <string>                                     
               "agent class name"
                    --agent_file <string>                     
               RandomAgent
               OptimalAgent
          --n_threads <integer>                                   
          [--compress_output]
          --output <string> 
          --refresh_frequency <integer>                          
          --backup_frequency <integer>                           

     --formula_set_generation
          [--seed <integer>]
          --n_variables <integer>
          --tokens <integer> <string> ... <string>
          --max_size <integer>
          [--reduce
               --n_points <integer>
               --points_range <double> <double>]
          [--compress_output]
          --output <string>

     --mdp_distrib_generation
          --name <string>
          --short_name <string>
          --n_states <integer>
          --n_actions <integer>
          --ini_state <integer>
          --transition_weights <double> ... <double>
          --reward_type <string>
          --reward_means <double> ... <double>
          [--reward_variances <double> ... <double>]
          --output <string>

     --help


OPTIONS DESCRIPTION
     --agent <string>
          The type of Agent to load among:
               - RandomAgent
               - OptimalAgent
               - EGreedyAgent (parameters: --epsilon)
               - SoftMaxAgent (parameters: --tau)
               - VDBEEGreedyAgent (parameters: --sigma, --delta, --ini_epsilon)
               - FormulaAgent (parameters: --formula, --variables)
               - BAMCPAgent (parameters: --K [--D])
               - BFS3Agent (parameters: --K --C [--D])
               - SBOSSAgent (parameters: --epsilon --delta)
               - BEBAgent (parameters: --beta)
               - OPPSDSAgent (parameters: --n_draws, --c, --formula_set, --variables, --discount_factor, --horizon_limit)
               - OPPSCSAgent (parameters: --n_eval, [--k --h_max --delta], --agent_factory, --discount_factor, --horizon_limit)

          Can either be followed by agent's parameters (if any) or by a file
          (--agent_file)

     --agent_factory <string>
          The type of AgentFactory to load among:
               - EGreedyAgentFactory (parameters: --epsilon, --counters)
               - SoftMaxAgentFactory (parameters: --tau, --counters)
               - VDBEEGreedyAgentFactory
                 (parameters: --sigma, --delta, --ini_epsilon, --counters)

     --agent_file <string>
          The file containing the data of the Agent to load.

     --backup_frequency <double>
          The frequency of backup creation (in s).

     --beta
          (BEBAgent parameter)
          The bonus parameter.

     --c <double>
          (OPPSDSAgent parameter)
          The constant to use in the UCB1 formula:
               I_t(i) = mu_i + c * sqrt(ln(n_t) / n_i)

     --compress_output
          If specified, the output/backup files are compressed.

     --C <integer>
          (BFS3Agent parameter)
          The branching factor.

     --delta <double>
          (OPPSCSAgent parameter)
          The confidence parameter in StoSOO.
          (if not specified: 'delta = 1 / sqrt(n_eval)')

     --delta <double>
          (SBOSSAgent parameter)
          The maximal threshold on the posterior deviation before resampling.

     --delta <double>
          (VDBEEGreedyAgent parameter)
          Determine the influence of the selected action on the exploration
          rate.

     --delta <double> <double>
          (VDBEEGreedyAgentFactory parameter)
          The minimal and maximal value of parameter 'delta' of the
          VDBEEGreedyAgents defined by this AgentFactory.

     --discount_factor <double>
          The discount factor.

     --D <integer>
          (BAMCPAgent & BFS3Agent parameter)
          The maximal depth.

     --epsilon <double>
          (EGreedyAgent parameter)
          The probability for the Agent to choose the next action to perform
          randomly.

     --epsilon <double>
          (SBOSS parameter)
          Defines the maximal error on sampling estimate.
     
     --epsilon <double> <double>
          (EGreedyAgentFactory parameter)
          The minimal and maximal value of parameter 'epsilon' of the
          EGreedyAgents defined by this AgentFactory.

     --experiment
          Followed by the parameters of the Experiment to load.

     --experiment_file <string>
          The file containing the data of the Experiment to load.

     --formula <string>
          (FormulaAgent parameter)
          A formula in RPN notation (between quotes to be parsed correctly).

     --formula_set
          (OPPSDSAgent parameter)
          Followed by the parameters of the formula set to load.

     --formula_set_file <string>
          (OPPSDSAgent parameter)
          The file containing the data of the formula set to load.

     --formula_set_generation
          Formula set Generation mode, where a formula set is created.

     --h_max
          (OPPSCSAgent parameter)
          The maximal depth of the tree in StoSOO.
          (if not specified: 'h_max = sqrt(n_eval / k)')

     --help
          Display this help.

     --horizon_limit <integer>
          The horizon limit.

     --ini_epsilon <double>
          (VDBEEGreedyAgent parameter)
          The initial value of epsilon for each state-action pair.

     --ini_state <integer>
          The initial state of the MDPs generated by the distribution.
          (-1: selected randomly for each trajectory)

     --k  <integer>
          (OPPSCSAgent parameter)
          The number of function evaluations per node in StoSOO.
          (if not specified: 'k = n_eval / log^3(n_eval)')

     --K <integer>
          (BAMCPAgent & BFS3Agent parameter)
          The number of nodes to expand at each time-step.

     --max_size <integer>
          The maximal size of a formula (= the maximal number of tokens to
          combine).
     
     --mdp_distribution <string>
          The type of MDP distribution to load among:
               - DirMultiDistribution

          Can either be followed by mdp distribution's parameters (if any) or
          by a file (--mdp_distribution_file)
     
     --mdp_distribution_file <string>
          The file containing the data of the MDP distribution to load.
     
     --mdp_distrib_generation
          MDP distribution Generation mode, where a mdp distribution is created.
     
     --name <string>
          The full name of the MDP distribution.
     
     --n_actions <integer>
          The number of actions of the MDPs generated by the distribution.
     
     --n_eval <integer>
          (OPPSCSAgent parameter)
          The number of function evaluations in StoSOO.
     
     --n_draws <integer>
          (OPPSDSAgent parameter)
          The number of draws of the UCB1.
          
     --n_mdps <integer>
          The number of MDPs to consider.
     
     --n_points <integer>
          The number of points to draw to discriminate the formulas.
     
     --n_simulations_per_mdp <integer>
          The number of trajectories per MDP to consider.

     --n_states <integer>
          The number of states of the MDPs generated by the distribution.

     --n_threads <integer>
          The number of threads to use.

     --n_variables <integer>
          The number of variables part of the set of tokens.

     --name <string>
          The name of the Experiment to create.

     --new_experiment
          New Experiment mode, where an Experiment is created.

     --offline_learning
          Offline Learning mode, where an Agent learns from a prior MDP
          distribution.
     
     --output <string>
          The output file.
     
     --points_range <double> <double>
          The range of value in which to draw each component of the points.
     
     --reduce
          If specified, reduce the set of formulas to generate by discarding
          the formulas which are equivalent.
          A formula F1 is equivalent to F2 iff any couple of points are sorted
          in the same way by both formula:
               F1(x1) < F1(x2) <--> F2(x1) < F2(x2)
                                or
               F1(x1) > F1(x2) <--> F2(x1) > F2(x2)
     
     --refresh_frequency <integer>
          The frequency of screen output refreshing (in s).

     --reward_type <string>
          The type of reward to load among:
               - RT_CONSTANT
               - RT_GAUSSIAN (requires '--rewards_variances' option)

     --reward_means <double> ... <double>
          Define, for each transition, the mean of the reward received by the
          agent.
          
          The means are listed by:
               (i) action, (ii) initial state, (iii) reached state.

          For example, given 3 states and 2 actions, the means of the rewards
          are defined in this order:
               <0, 0, 0> <0, 0, 1> <0, 0, 2>
               <1, 0, 0> <1, 0, 1> <1, 0, 2>
               <2, 0, 0> <2, 0, 1> <2, 0, 2>
               <0, 1, 0> <0, 1, 1> <0, 1, 2>
               <1, 1, 0> <1, 1, 1> <1, 1, 2>
               <2, 1, 0> <2, 1, 1> <2, 1, 2>

     --reward_variances <double> ... <double>
          Define, for each transition, the variance of the reward received by
          the agent.
          
          The variances are listed by:
               (i) action, (ii) initial state, (iii) reached state.

          For example, given 3 states and 2 actions, the variances of the
          rewards are defined in this order:
               <0, 0, 0> <0, 0, 1> <0, 0, 2>
               <1, 0, 0> <1, 0, 1> <1, 0, 2>
               <2, 0, 0> <2, 0, 1> <2, 0, 2>
               <0, 1, 0> <0, 1, 1> <0, 1, 2>
               <1, 1, 0> <1, 1, 1> <1, 1, 2>
               <2, 1, 0> <2, 1, 1> <2, 1, 2>

     --run_experiment
          Run Experiment mode, where an Agent is tested on a set of MDPs
          defined by an Experiment created previously.
     
     --safe_simulations
          If set, the MDP is 'unknown', preventing the agent to access MDP data
          (e.g.: the transition matrix).

     --save_trajectories
          If set, the created experiment will not save the complete trajectories
          of the agents tested on it (only the rewards will remain).

     --seed <integer>
          The seed to use for initializing the RNG.
          (<= 0 or unspecified: random seed)

     --short_name <string>
          The short name of the MDP distribution.

     --sigma <double>
          (VDBEEGreedyAgent parameter)
          The inverse sensitivity.
     
     --sigma <double> <double>
          (VDBEAgentFactory parameter)
          The minimal and maximal value of parameter 'sigma' of the
          VDBEEGreedyAgents defined by this AgentFactory.
          
     --tau
          (SoftMaxAgent parameter)
          The temperature, controlling smoothly the transition from a greedy
          behaviour (tau -> 0) to a random behaviour (tau -> inf), where the
          action which is the best according to the Agent is more likely to be
          drawn.

     --tau <double> <double>
          (SoftMaxAgentFactory parameter)
          The minimal and maximal value of parameter 'tau' of the
          SoftMaxAgents defined by this AgentFactory.

     --tokens <integer> <string> ... <string>
          Defines the set of tokens (except variables) to be used in the
          formula. The first integer is the number of tokens entered. The
          following strings are the list of symbols representing the tokens:
               <integer>    -->  A constant              (0 operand)   
               ABS          -->  The absolute value      (1 operand)
               LN           -->  The napierian logarithm (1 operand)
               SQRT         -->  The square root         (1 operand)
               INV          -->  The inverse             (1 operand)
               OPP          -->  The opposite            (1 operand)
               SUB          -->  The subtraction         (2 operands)
               DIV          -->  The division            (2 operands)
               ADD<integer> -->  The addition            (<integer> operand(s))
               MUL<integer> -->  The multiplication      (<integer> operand(s))
               MIN<integer> -->  The minimum             (<integer> operand(s))
               MAX<integer> -->  The maximum             (<integer> operand(s))
               AVG<integer> -->  The average             (<integer> operand(s))

     --transition_weights <double> ... <double>
          Define, for each transition, a concentration parameter of the
          Dirichlet multinomial distribution. They corresponds to the a priori
          observations.

          The concentration parameters are listed by:
               (i) action, (ii) initial state, (iii) reached state.

          For example, given 3 states and 2 actions, the concentration
          parameters are defined in this order:
               <0, 0, 0> <0, 0, 1> <0, 0, 2>
               <1, 0, 0> <1, 0, 1> <1, 0, 2>
               <2, 0, 0> <2, 0, 1> <2, 0, 2>
               <0, 1, 0> <0, 1, 1> <0, 1, 2>
               <1, 1, 0> <1, 1, 1> <1, 1, 2>
               <2, 1, 0> <2, 1, 1> <2, 1, 2>

     --variables <integer> <string> ... <string>
          (FormulaAgent, OPPSDSAgent and FormulaAgentFactory parameter)
          Defines the set of variables to be used in the formula(s). The first
          integer is the number of variables entered. The following variables
          are supported:
               QMean        --> Represents the Q-function based on the mean
                                model of a prior distribution.
               QSelf        --> Represents the Q-function based on the 'self'
                                model of a prior distribution (each state is
                                only reachable from itself).
               QUniform     --> Represents the Q-function based on the 'uniform'
                                model of a prior distribution (each state is
                                reachable from the other states).

          e.g.: --variables 2 "QUniform" "QMean"
                         ==>  X0 represents "QUniform"; X1 represents "QMean".

     --weights <double> <double>
          The minimal and maximal value of the weights of the FormulaAgents
          defined by this AgentFactory.
          
     

USAGE EXAMPLES

     --- Offline Learning mode ---
     
          ./BBRL-DDS --offline_learning \
               --agent EGreedyAgent --epsilon 0.25 \
               --mdp_distribution "DirMultiDistribution" \
                    --mdp_distribution_file "data/distributions/distrib.dat" \
               --output "data/agents/agent.dat"
     
     
     --- New Experiment mode ---
     
          ./BBRL-DDS --new_experiment \
               --name "GC Experiment" \
               --mdp_distribution "DirMultiDistribution" \
                    --mdp_distribution_file "data/distributions/distrib.dat" \
               --n_mdps 1000 --n_simulations_per_mdp 1 \
               --discount_factor 0.95 --horizon_limit 250 \
               --compress_output \
               --output "data/experiments/exp.dat"
     
     
     --- Run Experiment mode ---

          ./BBRL-DDS --run_experiment \
               --experiment \
                    --experiment_file "data/experiments/exp.dat.zz" \
               --agent EGreedyAgent \
                    --agent_file "data/agents/agent.dat" \
               --n_threads 1 --compress_output \
               --output "data/results/agent-exp.dat" \
               --refresh_frequency 1 --backup_frequency 15


     --- Formula set generation mode ---

          ./BBRL-DDS --formula_set_generation \
               --n_variables 3 \
               --tokens 4 ADD2 42 AVG3 MUL2 \
               --max_size 5 \
               --reduce --n_points 1000 --points_range -100.0 100.0 \
               --output "data/formula_sets/F.dat"


     --- MDP distribution generation mode ---

          ./BBRL-DDS --mdp_distrib_generation \
               --name "GChain distribution" \
               --short_name "GC" \
               --n_states 5 --n_actions 3 \
               --ini_state 0 \
               --transition_weights \
                    1 1 0 0 0 \
                    1 0 1 0 0 \
                    1 0 0 1 0 \
                    1 0 0 0 1 \
                    1 0 0 0 1 \
                    1 1 0 0 0 \
                    1 0 1 0 0 \
                    1 0 0 1 0 \
                    1 0 0 0 1 \
                    1 0 0 0 1 \
                    1 1 0 0 0 \
                    1 0 1 0 0 \
                    1 0 0 1 0 \
                    1 0 0 0 1 \
                    1 0 0 0 1 \
               --reward_type "RT_CONSTANT" \
               --reward_means \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
               --output "data/distributions/distrib.dat"

